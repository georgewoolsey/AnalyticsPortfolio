[["index.html", "Analytics Portfolio Chapter 1 Introduction 1.1 This book", " Analytics Portfolio George Woolsey 2022-03-20 Chapter 1 Introduction My name is George Woolsey and I am currently pursuing on a MS in Forest Sciences at Colorado State University. I am working with Dr. Wade Tinkham in the Forest Biometrics Lab. Our lab utilizes a combination of field inventories, remote sensing, GIS, and forest growth/landscape change models to characterize and address a wide array of applied forest management questions. 1.1 This book This book of selected analytics projects was compiled using R Markdown and the bookdown package which can be installed from CRAN or Github (code block below). The majority of these projects were completed as part of the Environmental Data Science course in the spring of 2022. install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) "],["intro.html", "Chapter 2 Using R Markdown 2.1 Methods 2.2 Assignment", " Chapter 2 Using R Markdown This chapter demonstrates some techniques for utilizing R Markdown to compile and share data analyses. The R Markdown: The Definitive Guide (Xie, Allaire, &amp; Grolemund 2018) was the primary reference for compiling this chapter and is an excellent resource for continued work in R Markdown. 2.1 Methods The Poudre River at Lincoln Bridge is: Downstream of only a little bit of urban stormwater Near Odell Brewing CO Near an open space area and the Poudre River Trail Downstream of many agricultral diversions 2.1.1 Site Description ### Data Acquisition and Plotting tests 2.1.2 Data Download ## pull stream flow data using dataRetrieval package q &lt;- readNWISdv(siteNumbers = &#39;06752260&#39;, parameterCd = &#39;00060&#39;, startDate = &#39;2017-01-01&#39;, endDate = &#39;2022-01-01&#39;) %&gt;% rename(q = &#39;X_00060_00003&#39;) 2.1.3 Static Data Plotter ## ggplot line chart ggplot(q, aes(x = Date, y = q)) + geom_line() + ylab(&#39;Q (cfs)&#39;) + ggtitle(&#39;Discharge in the Poudre River, Fort Collins&#39;) 2.1.4 Interactive Data Plotter ## transform data to xts object q_xts &lt;- xts(q$q, order.by = q$Date) ## dygraph to make interactive line chart dygraph(q_xts, main = &quot;Discharge in the Poudre River, Fort Collins, CO&quot;) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) 2.2 Assignment Fork the example repository into your personal GitHub Create an RStudio project from your Personal clone of the Repo. Create a table of contents that is floating, but displays three levels of headers instead of two (by editing the content at the beginning of the document) Make a version of the dygraph with points and lines by using rstudio’s dygraph guide Writing a paragraph on the Poudre river with at least three hyperlinks, two bolded sections, and one italicized phrase. The content of this paragraph is not vital, but try to at least make it true and interesting, and, of course, don’t plagiarize. Knit that document, and then git commit and push to your personal GitHub. Use the GitHub -&gt; Settings -&gt; Pages tab to create a website of your report. Bonus, make the timestamp in the header dynamic. As in it only adds todays date, not just a static date you enter. Bonus, create an “index_talk.Rmd” version of your document using the revealjs package. Add link to your original report-style document. 2.2.1 DyGraph example 2.2.1.1 Interact with this chart! ## transform data to xts object q_xts &lt;- xts(q$q, order.by = q$Date) ## dygraph to make interactive line chart dygraph(q_xts, main = &quot;Discharge in the Poudre River, Fort Collins, CO&quot;) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) %&gt;% dyOptions(drawPoints = TRUE, pointSize = 2) 2.2.2 Poudre River Overview The Cache la Poudre River in Larimer County, Colorado is one of 55 designated National Heritage Areas in the United States. The river showcases historical developments built by early Euro-American settlers, a popular trail system which follows the river through Fort Collins, and miles of natural beauty which, despite increasing human development, remains prime wildlife habitat. The Bird Conservancy of the Rockies has organized a community science project to study Eastern Screech-Owls along the Cache la Poudre River in Fort Collins and is currently seeking volunteer surveyors for the period March to mid-May. For those interested in the water flow dynamics of the river, there are several stream gauges maintained by Colorado Division of Water Resources with open access to data. 2.2.2.1 Map of Poudre River Trail System 2.2.3 Link to presentation-style webpage Go to presentation "],["wildfire-recovery.html", "Chapter 3 Wildfire Recovery 3.1 Read Data 3.2 Assignment", " Chapter 3 Wildfire Recovery 3.1 Read Data ####-----Reading in Data and Stacking it ----- #### #Reading in files files &lt;- list.files(&#39;data&#39;,full.names=T) files &lt;- files[grep(&quot;hayman&quot;, files, ignore.case=FALSE)] #Read in individual data files ndmi &lt;- read_csv(&quot;data/hayman_ndmi.csv&quot;) %&gt;% ## read_csv(files[1]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndmi&#39;) ndsi &lt;- read_csv(&quot;data/hayman_ndsi.csv&quot;) %&gt;% ## read_csv(files[2]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndsi&#39;) ndvi &lt;- read_csv(&quot;data/hayman_ndvi.csv&quot;) %&gt;% ## read_csv(files[3]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndvi&#39;) ## Stack as a tidy dataset full_long &lt;- rbind(ndvi,ndmi,ndsi) %&gt;% gather(key=&#39;site&#39;,value=&#39;value&#39;,-DateTime,-data) %&gt;% filter(!is.na(value)) 3.2 Assignment 3.2.1 Question 1) What is the correlation between NDVI and NDMI? - here I want you to convert the full_long dataset in to a wide dataset using the function “spread” and then make a plot that shows the correlation as a function of if the site was burned or not (x axis should be ndmi) You should exclude winter months and focus on summer months 3.2.1.1 Daily NDVI vs NDMI #use spread to pivot data from long to wide full_wide &lt;- full_long %&gt;% spread(key=data, value=value) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) #plot NDVI vs NDMI daily full_wide %&gt;% filter(month %in% (6:10)) %&gt;% ggplot(., aes(x = ndmi, y = ndvi, color=site)) + geom_point(alpha=0.5) + labs( title = &quot;Daily relationship between NDVI and NDMI (summer only)&quot; , subtitle = &quot;burned versus unburned area&quot; ) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , legend.title = element_blank() ) Based on the chart above, there is a positive correlation between the daily Normalized Difference Moisture Index (NDMI) and vegetation health as measured by NDVI in the summer months (June-October). There is a regime shift in the NDVI values of the burned area between pre-fire (higher NDVI values) and post-fire (lower NDVI values) measurements. 3.2.1.2 Monthly NDVI vs NDMI #aggregate data to monthly level and plot full_wide %&gt;% filter(month %in% (6:10)) %&gt;% group_by(site, year, month) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) %&gt;% ggplot(., aes(x = ndmi, y = ndvi, color=site)) + geom_point(alpha=0.5) + geom_smooth(method = &#39;lm&#39;) + labs( title = &quot;Monthly relationship between NDVI and NDMI (summer only)&quot; , subtitle = &quot;burned versus unburned area&quot; ) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , legend.title = element_blank() ) Based on the average NDVI and NDMI values at the monthly level presented above for summer months only (June-October), there is a more clear distinction between burned and unburned areas. The burned area vegetation health (NDVI) responds more positively to changes in moisture (NDMI) compared to the unburned area. 3.2.2 Question 2) What is the correlation between average NDSI (normalized snow index) for January - April and average NDVI for June-August? In other words, does the previous year’s snow cover influence vegetation growth for the following summer? ## aggregate data to yearly mean by season seas_yr_mean &lt;- full_long %&gt;% mutate(month = month(DateTime), year = year(DateTime)) %&gt;% filter(month %in% (1:4) | month %in% (6:8)) %&gt;% mutate(season = case_when( month %in% (1:4) ~ &quot;winter&quot; , month %in% (6:8) ~ &quot;summer&quot; , TRUE ~ &quot;other&quot;) ) %&gt;% group_by(site, data, year, season) %&gt;% summarize(mean_value = mean(value, na.rm = TRUE)) %&gt;% pivot_wider(names_from = c(data, season), names_sep = &quot;_&quot;, values_from = mean_value) ## plot relationship between winter snow cover (ndsi) and summer vegetation growth (ndvi) ggplot(seas_yr_mean, aes(x = ndsi_winter, y = ndvi_summer, color = site)) + geom_point(alpha=0.8) + labs( title = &quot;Relationship between winter snow cover (ndsi) and summer vegetation growth (ndvi)&quot; , subtitle = &quot;burned versus unburned area&quot; ) + xlab(&quot;NDSI winter (Jan-Apr)&quot;) + ylab(&quot;NDVI summer (Jun-Aug)&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , legend.title = element_blank() ) Based on the data presented above, there is not a clear relationship between snow as measured by NDSI in the winter (Jan-Apr) and vegetation health as measured by NDVI in the summer (Jun-Aug). This could be due to the vegetation species present in this area which is predominately slow-growing, conifer forest. 3.2.3 Question 3) How is the snow effect from question 2 different between pre- and post-burn and burned and unburned? ## create pre- and post-fire group var seas_yr_mean &lt;- seas_yr_mean %&gt;% mutate(pre_post_fire = case_when( year &lt; 2002 ~ &quot;pre-fire&quot; , year &gt; 2002 ~ &quot;post-fire&quot; , TRUE ~ &quot;other&quot;) ) ## plot relationship between winter snow cover (ndsi) and summer vegetation growth (ndvi) seas_yr_mean %&gt;% filter(pre_post_fire != &quot;other&quot;) %&gt;% ggplot(., aes(x = ndsi_winter, y = ndvi_summer, color = pre_post_fire)) + geom_point(alpha=0.8) + facet_grid(. ~ site) + labs( title = &quot;Relationship between winter snow cover (ndsi) and summer vegetation growth (ndvi)&quot; , subtitle = &quot;pre- versus post-fire in burned and unburned area&quot; ) + xlab(&quot;NDSI winter (Jan-Apr)&quot;) + ylab(&quot;NDVI summer (Jun-Aug)&quot;) + scale_color_manual(values = rev(brewer.pal(n=12, name=&quot;Paired&quot;)[c(2,12)])) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , legend.title = element_blank() ) Based on the data above, there does not appear to be a strong relationship between snow in the winter and vegetation health in the summer. The lack of relationship holds even when separating the influences of pre- and post-fire and burned and unburned area. Again, there is a regime shift in the NDVI values of the burned area in the post-fire measurements which are significantly lower than pre-fire conditions. 3.2.4 Question 4) What month is the greenest month on average? 3.2.4.1 Quick monthly NDVI chart ## quick bar chart for average ndvi ggplot(full_long %&gt;% filter(data==&quot;ndvi&quot;), aes(as.factor( month(DateTime) ), value, )) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, na.rm = TRUE, fill = &quot;#7FCDBB&quot;, width = 0.7) + scale_x_discrete(labels=month.abb) + labs( title = &quot;Overall average NDVI by month&quot; ) + xlab(&quot;&quot;) + ylab(&quot;NDVI (mean)&quot;) + theme_bw() Overall, August is typically the greenest month based on average NDVI. It is possible that this impact changes when looking at the impacts of the fire in burned and unburned areas (see below). 3.2.4.2 Detailed monthly NDVI chart ## summarize data by month, site, pre- post-fire means_month &lt;- full_long %&gt;% mutate(month = month(DateTime), year = year(DateTime)) %&gt;% mutate(pre_post_fire = case_when( year &lt; 2002 ~ &quot;pre-fire&quot; , year &gt; 2002 ~ &quot;post-fire&quot; , TRUE ~ &quot;other&quot;) ) %&gt;% group_by(site, data, month, pre_post_fire) %&gt;% summarize(mean_value = mean(value, na.rm = TRUE)) %&gt;% pivot_wider(names_from = c(data), values_from = mean_value) ## plot average NDVI over month by burned vs unburned means_month %&gt;% filter(pre_post_fire != &quot;other&quot;) %&gt;% ggplot(., aes(x = as.factor(month), y = ndvi, fill = pre_post_fire)) + geom_col(alpha=0.8, position = &quot;dodge2&quot;, width = 0.7) + facet_grid(. ~ site) + scale_x_discrete(labels=month.abb) + labs( title = &quot;Average NDVI by month&quot; , subtitle = &quot;pre- versus post-fire in burned and unburned area&quot; ) + ylab(&quot;NDVI (mean)&quot;) + xlab(&quot;&quot;) + scale_fill_manual(values = rev(brewer.pal(n=12, name=&quot;Paired&quot;)[c(2,12)])) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , legend.title = element_blank() , axis.text.x = element_text(size = 11, angle = 90, vjust=0.2) ) When separating the effects of the fire on NDVI in burned and unburned areas, the monthly trend in vegetation health is similar to the overall results. However, in the unburned area the greenest month shifts from August in pre-fire measurements to September in post-fire measurements. 3.2.5 Question 5) What month is the snowiest on average? 3.2.5.1 Quick monthly NDSI chart ## quick bar chart for average ndsi ggplot(full_long %&gt;% filter(data==&quot;ndsi&quot;), aes(as.factor( month(DateTime) ), value)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;, na.rm = TRUE, fill = &quot;#1D91C0&quot;, width = 0.7) + scale_x_discrete(labels=month.abb) + labs( title = &quot;Overall average NDSI by month&quot; ) + xlab(&quot;&quot;) + ylab(&quot;NDSI (mean)&quot;) + scale_y_continuous(breaks=seq(-0.5,0.2,0.1)) + theme_bw() Overall, January is typically the snowiest month based on average NDSI. 3.2.6 Bonus Question 1) Redo all problems with spread and gather using modern tidyverse syntax. The modern tidyverse syntax for reshaping data from long to wide is pivot_wider() and the syntax for reshaping data from wide to long is pivot_longer(). This syntax is utilized above in the section summarizing data by month, site, and a pre- post-fire grouping variable: # summarize data by month, site, pre- post-fire # use pivot_wider to transform data from long to wide means_month &lt;- full_long %&gt;% mutate(month = month(DateTime), year = year(DateTime)) %&gt;% mutate(pre_post_fire = case_when( year &lt; 2002 ~ &quot;pre-fire&quot; , year &gt; 2002 ~ &quot;post-fire&quot; , TRUE ~ &quot;other&quot;) ) %&gt;% group_by(site, data, month, pre_post_fire) %&gt;% summarize(mean_value = mean(value, na.rm = TRUE)) %&gt;% pivot_wider(names_from = c(data), values_from = mean_value) "],["web-scraping-and-functions.html", "Chapter 4 Web Scraping and Functions 4.1 Simple web scraping 4.2 Assignment", " Chapter 4 Web Scraping and Functions 4.1 Simple web scraping R can read html using either rvest, xml, or xml2 packages. Here we are going to navigate to the Center for Snow and Avalance Studies Website and read a table in. This table contains links to data we want to programatically download for three sites. We don’t know much about these sites, but they contain incredibly rich snow, temperature, and precipitation data. 4.1.1 Reading an html 4.1.1.1 Extract CSV links from webpage site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #See if we can extract tables and get the data that way tables &lt;- webpage %&gt;% html_nodes(&#39;table&#39;) %&gt;% magrittr::extract2(3) %&gt;% html_table(fill = TRUE) #That didn&#39;t work, so let&#39;s try a different approach #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;24hr&#39;,.)] %&gt;% html_attr(&#39;href&#39;) 4.1.2 Data Download 4.1.2.1 Download data in a for loop #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes dir.create(&quot;data&quot;, showWarnings = FALSE) file_names &lt;- paste0(&#39;data/&#39;,dataset) for(i in 1:3){ download.file(links[i],destfile=file_names[i]) } downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(downloaded) 4.1.2.2 Download data in a map #Map version of the same for loop (downloading 3 files) if(evaluate == T){ map2(links[1:3],file_names[1:3],download.file) }else{print(&#39;data already downloaded&#39;)} 4.1.3 Data read-in 4.1.3.1 Read in just the snow data as a loop #Pattern matching to only keep certain files snow_files &lt;- file_names %&gt;% .[!grepl(&#39;SG_24&#39;,.)] %&gt;% .[!grepl(&#39;PTSP&#39;,.)] #empty_data &lt;- list() ## snow_data &lt;- for(i in 1:length(snow_files)){ ## empty_data[[i]] &lt;- read_csv(snow_files[i]) %&gt;% ## select(Year,DOY,Sno_Height_M) ## } #snow_data_full &lt;- do.call(&#39;rbind&#39;,empty_data) #summary(snow_data_full) 4.1.3.2 Read in the data as a map function ## define a function to read in list of csv files our_snow_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_24hr.csv&#39;,&#39;&#39;,.) df &lt;- read_csv(file) %&gt;% select(Year,DOY,Sno_Height_M) %&gt;% mutate(site = name) } ## use map to call function defined above snow_data_full &lt;- map_dfr(snow_files,our_snow_reader) summary(snow_data_full) 4.1.3.3 Plot snow data ## summarize snow data by year snow_yearly &lt;- snow_data_full %&gt;% group_by(Year,site) %&gt;% summarize(mean_height = mean(Sno_Height_M,na.rm=T)) ## plot snow data ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + geom_point() + ggthemes::theme_few() + ggthemes::scale_color_few() 4.2 Assignment 4.2.1 Question 1 Extract the meteorological data URLs. Here we want you to use the rvest package to get the URLs for the SASP forcing and SBSP_forcing meteorological datasets. #use the rvest package to scrape the url site_url &lt;- &quot;https://snowstudies.org/archived-data/&quot; webpage &lt;- read_html(site_url) #find weblinks and extract the urls links &lt;- webpage %&gt;% html_nodes(&quot;a&quot;) %&gt;% ## &lt;a&gt; tag defines a hyperlink html_attr(&quot;href&quot;) %&gt;% ## href is the URL the link goes to .[grepl(&quot;.txt&quot;, .)] %&gt;% ## only pull txt files .[grepl(&quot;SASP_Forcing|SBSP_Forcing&quot;, .)] ## could also use just &quot;_Forcing&quot; but question specifically asks for only SASP and SBSP (in case site is updated later to include a new _Forcing file) 4.2.2 Question 2 Download the meteorological data. Use the download_file and str_split_fixed commands to download the data and save it in your data folder. You can use a for loop or a map function. 4.2.2.1 Download with for loop ## extract dataset names splits &lt;- str_split_fixed(links, &quot;/&quot;, n = 8) dataset &lt;- splits[,8] file_names &lt;- paste0(&#39;data/&#39;,dataset) ## use download_file to create local copy of data with for loop for(i in 1:length(links)){ download.file(links[i],destfile=file_names[i]) } 4.2.2.2 Download with map function ## with map function downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(downloaded) if(evaluate == T){ map2(links[1:length(links)], file_names[1:length(links)], download.file) } else{print(&#39;data already downloaded&#39;)} 4.2.3 Question 3 Write a custom function to read in the data and append a site column to the data. ## this code grabs the variable names from the metadata pdf file headers &lt;- pdf_text(&#39;https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf&#39;) %&gt;% readr::read_lines(.) %&gt;% trimws(.) %&gt;% str_split_fixed(.,&#39;\\\\.&#39;,2) %&gt;% .[,2] %&gt;% .[1:26] %&gt;% str_trim(side = &quot;left&quot;) %&gt;% make.names(unique = FALSE, allow_ = TRUE) ## create function to read data my_data_reader &lt;- function(file){ ## extract name of file fname &lt;- substr(file, str_locate(file, &quot;SBB_&quot;)[2]+1, str_locate(file, &quot;SBB_&quot;)[2]+4) ## read in data df &lt;- read_table(file , skip = 4 , col_names = headers , skip_empty_rows = TRUE , na = c(&quot;-9999.000&quot;, &quot;&quot;) ) %&gt;% select(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;air.temp..K.&quot;, &quot;precip..kg.m.2.s.1.&quot;) %&gt;% mutate(site = fname) return(df) } 4.2.4 Question 4 Use the map function to read in both meteorological files. Display a summary of your tibble. ## read in data using function created above metoc_hourly_full &lt;- map_dfr(file_names, my_data_reader) ## show data summary summary(metoc_hourly_full) ## year month day hour ## Min. :2003 Min. : 1.000 Min. : 1.00 Min. : 0.00 ## 1st Qu.:2005 1st Qu.: 3.000 1st Qu.: 8.00 1st Qu.: 5.75 ## Median :2007 Median : 6.000 Median :16.00 Median :11.50 ## Mean :2007 Mean : 6.472 Mean :15.76 Mean :11.50 ## 3rd Qu.:2009 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:17.25 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :23.00 ## air.temp..K. precip..kg.m.2.s.1. site ## Min. :242.1 Min. :0.000e+00 Length:138336 ## 1st Qu.:265.8 1st Qu.:0.000e+00 Class :character ## Median :272.6 Median :0.000e+00 Mode :character ## Mean :272.6 Mean :3.838e-05 ## 3rd Qu.:279.7 3rd Qu.:0.000e+00 ## Max. :295.8 Max. :6.111e-03 4.2.5 Question 5 Make a line plot of mean temp by year by site (using the air temp [K] variable). Is there anything suspicious in the plot? Adjust your filtering if needed. This initial plot of mean hourly temperature by year shows a much lower average temperature for both sites in the first year of the data (2003). ## summarize mean temp by year, site metoc_site_year &lt;- metoc_hourly_full %&gt;% group_by(site, year) %&gt;% summarise(mean_temp = mean(air.temp..K., na.rm = TRUE) , cnt = n()) %&gt;% arrange(site, year) ## function to create line graph of temp by year for each site my_line_graph &lt;- function(my_data){ print( ggplot(my_data, aes(x = year, y = mean_temp, color = site)) + geom_line(alpha=0.8, size = 1.3) + geom_point(alpha=0.8, size = 1.3) + scale_x_continuous(limits = c(min(metoc_hourly_full$year), max(metoc_hourly_full$year)), breaks = seq(1900, 2200, by = 1)) + labs( title = &quot;Average Temperature by Year&quot; , subtitle = &quot;Swamp Angel Study Plot and Senator Beck Study Plot&quot; ) + ylab(&quot;Mean Temp. (K)&quot;) + xlab(&quot;&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , axis.text.x = element_text(size = 11) ) ) } ## plot with full data my_line_graph(metoc_site_year) Looking at a count of the records by year reveals that the first and last year of the data do not include a full year of data. As this is hourly data, a full year of data should have 365*24 = 8,760 records (8,784 records on leap year). ## investigate what is happening in 2003 by looking at record counts ggplot(metoc_site_year, aes(x= as.character(year), y=cnt, fill = site)) + geom_col(alpha=0.8, width = 0.7) + geom_text(aes(label = scales::comma(cnt)), colour = &quot;black&quot;, angle = 90, hjust=&quot;top&quot;) + facet_grid(. ~ site) + scale_y_continuous(labels = label_comma()) + labs( title = &quot;Number of hourly records by year&quot; , subtitle = &quot;Swamp Angel Study Plot and Senator Beck Study Plot&quot; ) + ylab(&quot;Count Obs.&quot;) + xlab(&quot;&quot;) + scale_fill_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.x = element_text(size = 11, angle = 90, vjust=0.2) ) We can filter out yearly records that have incomplete data and plot the average hourly temperature by year again. ## line graph of temp by year for each site filtered for complete data my_line_graph(metoc_site_year %&gt;% filter(cnt &gt;= 8760)) 4.2.6 Question 6 Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Snow Angel Study Plot? Hint: https://ggplot2.tidyverse.org/reference/print.ggplot.html ## summarize mean temp by year, month, site metoc_site_year_mo &lt;- metoc_hourly_full %&gt;% group_by(site, year, month) %&gt;% summarise(mean_temp = mean(air.temp..K., na.rm = TRUE) , cnt = n()) %&gt;% arrange(site, year, month) ## function to create line graph of temp by year for each site my_line_graph_month &lt;- function(my_data, my_year){ #generate title my_title &lt;- paste0(as.character(my_year), &quot; Average Temperature by Month&quot;) #plot ( ggplot(my_data %&gt;% filter(year==my_year), aes(x = (month), y = mean_temp, color = site)) + geom_line(alpha=0.8, size = 1.3) + geom_point(alpha=0.8, size = 1.3) + scale_x_discrete(limits=month.abb) + labs( title = my_title , subtitle = &quot;Swamp Angel Study Plot and Senator Beck Study Plot&quot; ) + ylab(&quot;Mean Temp. (K)&quot;) + xlab(&quot;&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , axis.text.x = element_text(size = 11) ) ) } ## plot with loop to create a plot for each year my_years &lt;- c(2005:2010) for (i in 1:length(my_years)) { print( my_line_graph_month(metoc_site_year_mo, my_years[i]) ) } 4.2.6.1 A better way? Producing an individual chart for each year of interest can also be accomplished by using the facet_grid option in ggplot. With this option there is no need to write a loop to produce an individual graph for each year. #plot each year using facet_grid ggplot(metoc_site_year_mo %&gt;% filter(year %in% my_years), aes(x = (month), y = mean_temp, color = site)) + geom_line(alpha=0.8, size = 0.9) + geom_point(alpha=0.8, size = 0.9) + facet_grid(. ~ year) + scale_x_discrete(limits=month.abb) + labs( title = &quot;2005-2010 Average Temperature by Month&quot; , subtitle = &quot;Swamp Angel Study Plot and Senator Beck Study Plot&quot; ) + ylab(&quot;Mean Temp. (K)&quot;) + xlab(&quot;&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;bottom&quot; , legend.box = &quot;horizontal&quot; , axis.text.x = element_text(size = 7, angle = 90) ) 4.2.7 Bonus Question 1 Make a plot of average daily precipitation by day of year (averaged across all available years). Color each site. ## summarize total precipitation by date, site metoc_doy_site &lt;- metoc_hourly_full %&gt;% mutate(date_id = make_date(year, month, day) , day_of_year = yday(date_id) ) %&gt;% group_by(site, date_id, day_of_year) %&gt;% summarise(tot_precip = sum(precip..kg.m.2.s.1.)) %&gt;% group_by(site, day_of_year) %&gt;% ## now take the avg of total daily precip by doy summarise(mean_daily_precip = mean(tot_precip, na.rm = TRUE)) %&gt;% arrange(site, day_of_year) %&gt;% mutate(month_day = as.Date(day_of_year, origin = &quot;1999-12-31&quot;)) ## this date will make for plotting with date label ## note to choose a leap year above to handle years with 366 days ## plot mean precip by doy ggplot(metoc_doy_site, aes(x = month_day, y = mean_daily_precip, color = site)) + geom_line(alpha=0.5, size = 0.5) + geom_point(alpha=0.5, size = 0.4) + geom_smooth(method = loess, se = FALSE, alpha=1, size = 0.9) + facet_grid(. ~ site) + scale_x_date(date_labels = &quot;%b-%d&quot;, date_minor_breaks = &quot;1 month&quot;) + labs( title = &quot;Average Precipitation by Day of Year&quot; , subtitle = &quot;Swamp Angel Study Plot and Senator Beck Study Plot&quot; ) + ylab(&quot;Mean Precip. (kg/m2/s)&quot;) + xlab(&quot;&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.x = element_text(size = 11, angle = 90) ) 4.2.8 Bonus Question 2 Use a function and for loop to create yearly plots of precipitation by day of year. Color each site. Personally, I prefer using facet_grid to make small multiples instead of creating an independent graph for each year. ## summarize total precipitation by date, site metoc_hourly_full %&gt;% filter(year %in% c(2004:2010)) %&gt;% mutate(date_id = make_date(year, month, day) , day_of_year = yday(date_id) ) %&gt;% group_by(site, day_of_year, year) %&gt;% summarise(tot_precip = sum(precip..kg.m.2.s.1.)) %&gt;% arrange(site, year, day_of_year) %&gt;% mutate(month_day = as.Date(day_of_year, origin = &quot;1999-12-31&quot;)) %&gt;% ## this date will make for plotting with date label ## plot mean precip by doy ggplot(., aes(x = month_day, y = tot_precip, color = site)) + geom_line(alpha=0.5, size = 0.5) + geom_smooth(method = loess, span = 0.2, se = FALSE, alpha=1, size = 0.9) + facet_grid(year ~ site, scales = &quot;free_y&quot;) + scale_x_date(date_labels = &quot;%b-%d&quot;, date_minor_breaks = &quot;1 month&quot;) + labs( title = &quot;Precipitation by Day of Year&quot; , subtitle = &quot;Swamp Angel Study Plot and Senator Beck Study Plot&quot; ) + ylab(&quot;Tot. Precip. (kg/m2/s)&quot;) + xlab(&quot;&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.y = element_text(size = 6) , axis.text.x = element_text(angle = 90) ) "],["sp1.html", "Chapter 5 Spatial Analysis (part 1) 5.1 LAGOS Analysis 5.2 Assignment", " Chapter 5 Spatial Analysis (part 1) 5.1 LAGOS Analysis The r package ‘LAGOSNE’ aids users in accessing data from the Lake Multi-Scaled Geospatial and Temporal Database. 5.1.1 Loading in data 5.1.1.1 First download and then specifically grab the locus (or site coordinates) ## #Lagos download script LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T) #Load in lagos lagos &lt;- lagosne_load() names(lagos) #Grab the lake centroid info lake_centers &lt;- lagos$locus str(lake_centers) 5.1.1.2 Convert to spatial data #Look at the column names #names(lake_centers) #Look at the structure #str(lake_centers) #View the full dataset #View(lake_centers %&gt;% slice(1:100)) spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) %&gt;% ## EPSG:4326 = WGS 84 st_transform(2163) ## EPSG:2163 = US National Atlas Equal Area #Subset for plotting subset_spatial &lt;- spatial_lakes %&gt;% slice(1:100) ## subset_baser &lt;- spatial_lakes[1:100,] #Dynamic mapviewer mapview(subset_spatial) 5.1.1.3 Subset to only Minnesota ## load state polygon file from USAboundaries states &lt;- us_states() #Plot all the states to check if they loaded #mapview(states) minnesota &lt;- states %&gt;% filter(name == &#39;Minnesota&#39;) %&gt;% st_transform(2163) ## EPSG:2163 = US National Atlas Equal Area #Subset lakes based on spatial position minnesota_lakes &lt;- spatial_lakes[minnesota,] #Plotting the first 1000 lakes minnesota_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;) 5.2 Assignment 5.2.1 Question 1) Show a map outline of Iowa and Illinois (similar to Minnesota map upstream) #filter states dataset from USA USAboundaries us_states() il_ia &lt;- states %&gt;% filter(name %in% c(&quot;Illinois&quot;, &quot;Iowa&quot;)) %&gt;% st_transform(2163) ## EPSG:2163 = US National Atlas Equal Area #see names of basemaps available to pass to &quot;map.types&quot; option below ## names(leaflet.providers::providers_loaded()$providers) #make map mapview(il_ia , zcol = &quot;name&quot; , legend = FALSE , col.regions = turbo(n= min(5, n_distinct(il_ia$name))) , alpha.regions = 0.5 , map.types = &quot;OpenStreetMap&quot; , label = c(&quot;stusps&quot;) ) 5.2.2 Question 2) Subset LAGOS data to these sites, how many sites are in Illinois and Iowa combined? How does this compare to Minnesota? #Subset lakes based on spatial position il_ia_lakes &lt;- spatial_lakes[il_ia,] #create counts dataset cnt_lakes &lt;- c( n_distinct(il_ia_lakes$lagoslakeid) , n_distinct(minnesota_lakes$lagoslakeid) ) st &lt;- c(&quot;Iowa + Illinois&quot;, &quot;Minnesota&quot;) df &lt;- data.frame(cnt_lakes, st) #plot Iowa + Illinois Lakes vs. Minnesota Lakes ggplot(df, aes(x = st, y = cnt_lakes, fill = st)) + geom_col(alpha=0.85, width = 0.5) + geom_text(aes(label = scales::comma(cnt_lakes)), colour = &quot;black&quot;, size = 4, vjust = &quot;top&quot;) + scale_y_continuous(labels = label_comma()) + labs( title = &quot;Iowa + Illinois Lakes vs. Minnesota Lakes&quot; ) + xlab(&quot;&quot;) + ylab(&quot;## Lakes&quot;) + scale_fill_viridis(discrete = TRUE, option = &quot;cividis&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.y = element_text(size = 11) , axis.text.x = element_text(size = 11) ) There are 16,466 lakes in Iowa and Illinois combined while Minnesota has 29,038 lakes. There are 12,572 more lakes in Minnesota than in Iowa and Illinois combined. 5.2.3 Question 3) What is the distribution of lake size in Iowa vs. Minnesota? Here I want to see a histogram plot with lake size on x-axis and frequency on y axis (check out geom_histogram) ## get Minnesota and Iowa polygons mn_ia &lt;- states %&gt;% filter(name %in% c(&quot;Minnesota&quot;, &quot;Iowa&quot;)) %&gt;% st_transform(2163) ## EPSG:2163 = US National Atlas Equal Area ## spatial join points to polygons mn_ia_lakes &lt;- st_join(spatial_lakes, mn_ia, left = FALSE) %&gt;% mutate(lake_area_ha_log = log10(lake_area_ha)) ## count lakes ## n_distinct(mn_ia_lakes$lagoslakeid) #plot ggplot(mn_ia_lakes, aes(lake_area_ha_log, fill = name)) + geom_histogram(bins = 15, alpha = 0.8) + facet_grid(. ~ name) + scale_y_continuous(labels = label_comma()) + labs( title = &quot;Distribution of Lake Area in Minnesota and Iowa&quot; ) + xlab(&quot;log10 Lake Area (ha)&quot;) + ylab(&quot;## Lakes&quot;) + scale_fill_viridis(discrete = TRUE, option = &quot;cividis&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.y = element_text(size = 11) , axis.text.x = element_text(size = 11) ) This histogram uses the Log10 of lake area due to outliers at the high end of lake area. Below is a solution to bin the top 5% of lakes based on area into their own bucket in order to display the untransformed lake area on the x-axis. In addition, to get a better comparison between states, we can use the after_stat(density) option to plot density on the y-axis instead of count. ## truncate top 5% of lakes in area into own bin mn_ia_lakes &lt;- mn_ia_lakes %&gt;% mutate(lake_area_ha_trunc = ifelse( lake_area_ha &gt;= quantile(mn_ia_lakes$lake_area_ha, 1-0.05) , quantile(mn_ia_lakes$lake_area_ha, 1-0.05) , lake_area_ha) ) ## plot ggplot(mn_ia_lakes, aes(lake_area_ha_trunc, after_stat(density), fill = name)) + geom_histogram(bins = 30, alpha = 0.8) + facet_grid(. ~ name) + scale_y_continuous(labels = label_percent(accuracy = 1)) + labs( title = &quot;Distribution of Lake Area in Minnesota and Iowa&quot; ) + xlab(&quot;Lake Area (ha)&quot;) + ylab(&quot;% Lakes&quot;) + scale_fill_viridis(discrete = TRUE, option = &quot;cividis&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.y = element_text(size = 11) , axis.text.x = element_text(size = 11) , strip.text.x = element_text(size = 13) ) This histogram is more informative and allows us to see that Iowa has a higher propoertion of small lakes compared to Minnesota. There are many more large lakes (over ~84 ha) in Minnesota than in Iowa. 5.2.4 Question 4) Make an interactive plot of lakes in Iowa and Illinois and color them by lake area in hectares #sort lakes so that largest lakes are on bottom il_ia_lakes_filter &lt;- il_ia_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mutate(lake_area_ha_log = log10(lake_area_ha)) #point map mapview(il_ia_lakes_filter , zcol = &quot;lake_area_ha&quot; , cex = &quot;lake_area_ha_log&quot; , alpha.regions = 0.5 , map.types = &quot;OpenStreetMap&quot; , label = c(&quot;gnis_name&quot;) , layer.name = &quot;lake area (ha)&quot; , popup = popupTable( il_ia_lakes_filter , zcol = c( &quot;gnis_name&quot; , &quot;lake_area_ha&quot; , &quot;lagoslakeid&quot; ) , row.numbers = FALSE , feature.id = FALSE ) ) The interactive map of lakes is displaying only the largest 1000 lakes in Illinois and Iowa. Because there were 16,466 lakes in the two states, a filter is helpful in limiting the noise in the plot from too many points. 5.2.5 Question 5) What other data sources might we use to understand how reservoirs and natural lakes vary in size in these three states? The r package ‘LAGOSNE’ aids users in accessing data from the Lake Multi-Scaled Geospatial and Temporal Database. In addition to the locus dataset utilized above, the package also contains the lakes_limno dataset which includes measures of mean and maximum lake depth (meters). The package also makes the following datasets available: * chag datasets contain information on climate, hydrology, atmospheric deposition, and surficial geology characteristics. * conn datasets contain lake, stream, and wetland abundance and connectivity metrics. * lulc datasets contain information on the land use and land cover (LULC) characteristics. "],["sp2.html", "Chapter 6 Spatial Analysis (part 2) 6.1 LAGOS Analysis 6.2 Assignment", " Chapter 6 Spatial Analysis (part 2) 6.1 LAGOS Analysis The r package ‘LAGOSNE’ aids users in accessing data from the Lake Multi-Scaled Geospatial and Temporal Database. 6.1.1 Loading in data 6.1.1.1 First download and then specifically grab the locus (or site lat longs) If you didn’t keep the data downloaded in Part 1 then run the first line which is commented out. #Lagos download script ## LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus ## Make an sf object spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) #Grab the water quality data nutr &lt;- lagos$epi_nutr #Look at column names #names(nutr) 6.1.1.2 Subset columns nutr to only keep key info that we want ## keep only selected columns clarity_only &lt;- nutr %&gt;% select(lagoslakeid,sampledate,chla,doc,secchi) %&gt;% mutate(sampledate = as.character(sampledate) %&gt;% ymd(.)) 6.1.1.3 Keep sites with at least 200 observations #Look at the number of rows of dataset #nrow(clarity_only) chla_secchi &lt;- clarity_only %&gt;% filter(!is.na(chla), !is.na(secchi)) ## How many observatiosn did we lose? ## nrow(clarity_only) - nrow(chla_secchi) ## Keep only the lakes with at least 200 observations of secchi and chla chla_secchi_200 &lt;- chla_secchi %&gt;% group_by(lagoslakeid) %&gt;% mutate(count = n()) %&gt;% filter(count &gt; 200) 6.1.1.4 Join water quality data to spatial data ## join data spatial_200 &lt;- inner_join(spatial_lakes, chla_secchi_200 %&gt;% distinct(lagoslakeid, keep_all=T), by=&#39;lagoslakeid&#39;) 6.1.1.5 Mean Chl_a map #### Take the mean chl_a and secchi by lake mean_values_200 &lt;- chla_secchi_200 %&gt;% ## Take summary by lake id group_by(lagoslakeid) %&gt;% ## take mean chl_a per lake id summarize(mean_chl = mean(chla,na.rm=T), mean_secchi=mean(secchi,na.rm=T)) %&gt;% #Get rid of NAs filter(!is.na(mean_chl), !is.na(mean_secchi)) %&gt;% ## Take the log base 10 of the mean_chl mutate(log10_mean_chl = log10(mean_chl)) #Join datasets mean_spatial &lt;- inner_join(spatial_lakes,mean_values_200, by=&#39;lagoslakeid&#39;) #Make a map mapview(mean_spatial, zcol=&quot;log10_mean_chl&quot;, layer.name = &quot;log chlorophyll a (mcg per L)&quot;) 6.2 Assignment 6.2.1 Question 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations? Here, I just want a plot of chla vs secchi for all sites ## using the mean_values_200 dataset explore relationship of chla and secchi ggplot(mean_values_200, aes(x = mean_chl, y = mean_secchi)) + geom_point(alpha=0.8, color = &quot;navy&quot;, size = 2) + geom_smooth(method = &#39;lm&#39;, color = &quot;grey35&quot;) + labs( title = &quot;Relationship between Secchi Disk Transparency and Chlorophyll a&quot; , subtitle = &quot;for North East US lakes with at least 200 obs.&quot; ) + xlab(&quot;Chlorophyll a (mcg/L)&quot;) + ylab(&quot;Secchi disk transparency (m)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) ## Simple linear model Chlorophyll a (mcg per L) on Secchi disk transparency (m) lm_secchi_chla &lt;- lm(mean_secchi ~ mean_chl, data = mean_values_200) ## explore model results ## names(summary(lm_secchi_chla)) ## summary(lm_secchi_chla) ## summary(lm_secchi_chla)$coefficients[&quot;mean_chl&quot;, &quot;Estimate&quot;] ## summary(lm_secchi_chla)$adj.r.squared The relationship between Secchi disk transparency (m) and Chlorophyll a (mcg/L) in the lakes included in this analysis is negative. As Chlorophyll a concentration increases, the depth at which a Secchi disk can be seen decreases. Based on a simple linear regression for every one unit increase in Chlorophyll a (mcg/L), the Secchi disk transparency changes by -0.07m. The variation in Chlorophyll a explains 27% of the variation in Secchi disk transparency for lakes included in this analysis. 6.2.1.1 1a) Why might this be the case? A Secchi disk is a black and white disk that is lowered by hand into the water to the depth at which it vanishes from sight; the distance to vanishing is then recorded (US Environmental Protection Agency). The clearer the water, the greater the distance. Chlorophyll a is the predominant type of chlorophyll found in green plants and algae (US Environmental Protection Agency). The higher the concentration of plants and algae in a lake, the lower the sub-surface visibility. Blue-green algae bloom on the shore of Catawba Island, Ohio, in Lake Erie, summer 2009. Photo: NOAA. 6.2.2 Question 2) What states have the most data? 6.2.2.1 2a) First you will need to make a lagos spatial dataset that has the total number of counts per site. ## using the epi_nutr dataset filtered for observations that have ## non-null values in both chla and secchi columns ... ## row is unique by lagoslakeid and sampledate #check how row is unique ## nrow(chla_secchi) ## chla_secchi %&gt;% distinct(lagoslakeid) %&gt;% nrow(.) ## chla_secchi %&gt;% distinct(lagoslakeid, sampledate) %&gt;% nrow(.) ## summarize data by lake lake_clarity &lt;- chla_secchi %&gt;% group_by(lagoslakeid) %&gt;% summarize( mean_chl = mean(chla, na.rm=T) , mean_secchi=mean(secchi, na.rm=T) , count_obs = n() , first_date = min(sampledate) , last_date = max(sampledate) ) #count ## nrow(lake_clarity) 6.2.2.2 2b) Second, you will need to join this point dataset to the us_boundaries data. ## make lake clarity data created above spatial spatial_lake_clarity &lt;- spatial_lakes %&gt;% inner_join(lake_clarity, by = c(&quot;lagoslakeid&quot;)) %&gt;% st_transform(2163) ## EPSG:2163 = US National Atlas Equal Area #count ## nrow(spatial_lake_clarity) ## load in us states polygons states &lt;- us_states() %&gt;% st_transform(2163) ## EPSG:2163 = US National Atlas Equal Area ## spatial join points to polygons spatial_lake_clarity &lt;- st_join(spatial_lake_clarity, states, left = TRUE) ## note, there are lakes for which there are clarity measurements but the ## point location falls outside of the borders of the US polygons ## see map ## mapview(spatial_lake_clarity %&gt;% filter(is.na(statefp))) ## rename state if outside US border spatial_lake_clarity &lt;- spatial_lake_clarity %&gt;% mutate(state_name = ifelse(is.na(state_name), &quot;Outside US&quot;, state_name)) 6.2.2.3 2c) Then you will want to group by state and sum all the observations in that state and arrange that data from most to least total observations per state. ## summarize lake clarity data to state level state_lake_clarity &lt;- spatial_lake_clarity %&gt;% group_by(statefp, state_name) %&gt;% summarise( count_lakes_w_data = n() , count_clarity_measurements = sum(count_obs, na.rm = TRUE) , mean_clarity_measurements = mean(count_obs, na.rm = TRUE) , first_date = min(first_date) , last_date = max(last_date) , mean_chl_wgt_area = weighted.mean(mean_chl, w = lake_area_ha, na.rm = TRUE) , mean_secchi_wgt_area = weighted.mean(mean_secchi, w = lake_area_ha, na.rm = TRUE) ) %&gt;% arrange(-count_clarity_measurements) ## plot data ggplot(state_lake_clarity, aes(x=count_clarity_measurements, y = reorder(state_name, count_clarity_measurements), fill = count_clarity_measurements)) + geom_col(alpha=0.9, width = 0.6) + geom_text(aes(label = scales::comma(count_clarity_measurements, accuracy = 1)), colour = &quot;black&quot;, angle = 0, size = 3, hjust = &quot;left&quot;, nudge_x = 100) + scale_x_continuous(labels = label_comma(), limits = c(0, max(state_lake_clarity$count_clarity_measurements)*1.05) ) + labs( title = &quot;Number of Lake Clarity Measurements by State&quot; , subtitle = &quot;only observations with both Secchi and Chlorophyll measurements included&quot; ) + ylab(&quot;&quot;) + xlab(&quot;## Clarity Measurements&quot;) + scale_fill_viridis(option = &quot;viridis&quot;, direction = -1) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.y = element_text(size = 9) , axis.text.x = element_text(size = 11, angle = 0) ) Using the epi_nutr dataset filtered for observations that have non-null values in both the chla and secchi column, Minnesota is the state with the most lake clarity measurements with 78,455 observations covering 2,740 lakes. 6.2.2.4 2c) Bonus Count the unique lakes in each state with at least one clarity measurment ## plot ## lakes with clarity data ggplot(state_lake_clarity, aes(x=count_lakes_w_data, y = reorder(state_name, count_lakes_w_data), fill = count_lakes_w_data)) + geom_col(alpha=0.9, width = 0.6) + geom_text(aes(label = comma(count_lakes_w_data, accuracy = 1)), colour = &quot;black&quot;, angle = 0, size = 3, hjust = &quot;left&quot;, nudge_x = 20) + scale_x_continuous(labels = label_comma(), limits = c(0, max(state_lake_clarity$count_lakes_w_data)*1.07) ) + labs( title = &quot;Number of Lakes w/ at least one clarity measurement by State&quot; , subtitle = &quot;only observations with both Secchi and Chlorophyll measurements included&quot; ) + ylab(&quot;&quot;) + xlab(&quot;## Lakes w/ Clarity Measurements&quot;) + scale_fill_viridis(option = &quot;cividis&quot;, direction = -1) + theme_bw() + theme( legend.position=&quot;none&quot; , axis.text.y = element_text(size = 9) , axis.text.x = element_text(size = 11, angle = 0) ) 6.2.3 Question 3) Is there a spatial pattern in Secchi disk depth for lakes with at least 200 observations? mapviewOptions(legend.pos = &quot;bottomright&quot;, homebutton.pos = &quot;bottomleft&quot;) ## using the same dataset used to map chlorophyll a measurements mapview(mean_spatial , zcol = &quot;mean_secchi&quot; , cex = &quot;mean_secchi&quot; , alpha.regions = 0.5 , map.types = &quot;OpenStreetMap&quot; , label = c(&quot;gnis_name&quot;) , col.regions = mako(n = 6, direction = -1) , at = seq(0, 10, 2) , legend = TRUE , layer.name = &quot;Secchi disk transparency (m)&quot; , popup = popupTable( mean_spatial , zcol = c( &quot;gnis_name&quot; , &quot;mean_secchi&quot; , &quot;mean_chl&quot; ) , row.numbers = FALSE , feature.id = FALSE ) ) Yes, there appears to be a spatial pattern in Secchi disk transparency (m) for lakes with at least 200 observations. Based on the map above, lakes in the eastern states (e.g. New York, New Hampshire) generally have visibility of the Secchi disk at a deeper level than lakes in the western states (e.g. Minnesota, Missouri). "],["regression-analysis.html", "Chapter 7 Regression Analysis", " Chapter 7 Regression Analysis This analysis utilizes the The National Agricultural Statistics Service (NASS) Quick Stats data from an on-line database containing official published aggregate estimates related to U.S. agricultural production. 7.0.1 Weather Data Analysis 7.0.1.1 Load the PRISM daily maximum temperatures ## daily max temperature ## dimensions: counties x days x years prism &lt;- readMat(&quot;data/prismiowa.mat&quot;) ## look at county #1 t_1981_c1 &lt;- prism$tmaxdaily.iowa[1,,1] t_1981_c1[366] plot(1:366, t_1981_c1, type = &quot;l&quot;) ## plot temp by DOY ggplot() + geom_line(mapping = aes(x=1:366, y = t_1981_c1)) + theme_bw() + xlab(&quot;day of year&quot;) + ylab(&quot;daily maximum temperature (°C)&quot;) + ggtitle(&quot;Daily Maximum Temperature, Iowa County #1&quot;) 7.0.1.2 tidy weather data ## assign dimension names to tmax matrix dimnames(prism$tmaxdaily.iowa) &lt;- list(prism$COUNTYFP, 1:366, prism$years) ## converted 3d matrix into a data frame tmaxdf &lt;- as.data.frame.table(prism$tmaxdaily.iowa) ## relabel the columns colnames(tmaxdf) &lt;- c(&quot;countyfp&quot;,&quot;doy&quot;,&quot;year&quot;,&quot;tmax&quot;) tmaxdf &lt;- tibble(tmaxdf) 7.0.2 Temperature trends 7.0.2.1 Summer temperature trends: Winneshiek County ## transform data types tmaxdf$doy &lt;- as.numeric(tmaxdf$doy) tmaxdf$year &lt;- as.numeric(as.character(tmaxdf$year)) ## summarize summer temperature by year winnesummer &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ## plot temp by year ggplot(winnesummer, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) ## simple linear regression of summer temp on year lm_summertmax &lt;- lm(meantmax ~ year, winnesummer) summary(lm_summertmax) 7.0.2.2 Winter Temperatures - Winneshiek County ## summarize winter temperature data by year winnewinter &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; (doy &lt;= 59 | doy &gt;= 335) &amp; !is.na(tmax)) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ## plot yearly winter temperature ggplot(winnewinter, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) ## simple linear regression of winter temp on year lm_wintertmax &lt;- lm(meantmax ~ year, winnewinter) summary(lm_wintertmax) 7.0.2.3 Multiple regression – Quadratic time trend ## create squared term of year winnewinter$yearsq &lt;- winnewinter$year^2 ## quadratic regreassion of winter temp on year and year2 lm_wintertmaxquad &lt;- lm(meantmax ~ year + yearsq, winnewinter) summary(lm_wintertmaxquad) ## save fitted values winnewinter$fitted &lt;- lm_wintertmaxquad$fitted.values ## plot fitted values ggplot(winnewinter) + geom_point(mapping = aes(x = year, y = meantmax)) + geom_line(mapping = aes(x = year, y = fitted)) + theme_bw() + labs(x = &quot;year&quot;, y = &quot;tmax&quot;) 7.0.2.4 Download NASS corn yield data The rnassqs package requires a unique access key which can be obtained here. After acquiring this key, define it in your code session as the variable my_qs_api_key. ## set our API key with NASS nassqs_auth(key = my_qs_api_key) ## parameters to query on params &lt;- list(commodity_desc = &quot;CORN&quot;, util_practice_desc = &quot;GRAIN&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) ## download cornyieldsall &lt;- nassqs_yields(params) cornyieldsall$county_ansi &lt;- as.numeric(cornyieldsall$county_ansi) cornyieldsall$yield &lt;- as.numeric(cornyieldsall$Value) ## clean and filter this dataset cornyields &lt;- select(cornyieldsall, county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) cornyields &lt;- tibble(cornyields) %&gt;% rename(year_id = year , yield_corn_bu_acre = yield ## bushels per acre ) 7.0.3 Assignment 7.0.3.1 Question 1a Extract Winneshiek County corn yields, fit a linear time trend, make a plot. Is there a significant time trend? ## ## check out yield data structure ## str(cornyields) ## nrow(cornyields) ## cornyields %&gt;% distinct(county_ansi) %&gt;% nrow(.) ## cornyields %&gt;% distinct(county_ansi, year_id) %&gt;% nrow(.) ## ## row is unique by county, year ## filter to specific county winneshiek_yields &lt;- cornyields %&gt;% filter(toupper(county_name) == &quot;WINNESHIEK&quot;) ## trend model for yield growth lm_yield_year &lt;- lm(yield_corn_bu_acre ~ year_id , data = winneshiek_yields) summary(lm_yield_year)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;] ## label model in ggplot model_label &lt;- as.character(as.expression( substitute( italic(y) == a + b %.% italic(year)*&quot;,&quot;~~italic(r)^2~&quot;=&quot;~r2 , list( a = scales::comma(summary(lm_yield_year)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;], accuracy = 0.1) , b = scales::comma(summary(lm_yield_year)$coefficients[2, &quot;Estimate&quot;], accuracy = 0.1) , r2 = format(summary(lm_yield_year)$r.squared, digits = 2) ) ) )) ## model_label ## plot ggplot(winneshiek_yields, aes(x = year_id, y = yield_corn_bu_acre)) + geom_point(alpha=0.8, color = &quot;navy&quot;, size = 2) + geom_smooth(method = &#39;lm&#39;, color = &quot;grey35&quot;) + annotate(geom = &quot;text&quot;, label = model_label, parse = TRUE, x = -Inf, y = Inf, hjust = -0.1, vjust = 2) + scale_x_continuous(breaks = scales::extended_breaks(n=10)) + labs( title = &quot;Corn Yield over time in Winneshiek County, Iowa&quot; ) + xlab(&quot;Year&quot;) + ylab(&quot;Corn Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) There is a significant time trend in the corn yields in Winneshiek County, Iowa from 1981 to 2021. Based on a simple linear regression, every additional year results in the corn yield increasing by 2.46 bu/acre. The annual time trend explains 76% of the variation in corn yields in Winneshiek County, Iowa. 7.0.3.2 Question 1b Fit a quadratic time trend (i.e., year + year^2) and make a plot. Is there evidence for slowing yield growth? ## quadratic trend model for evidence of slowing yield growth winneshiek_yields &lt;- winneshiek_yields %&gt;% mutate(year_id_sq = year_id^2) ## run model lm_yield_year_quad &lt;- lm(yield_corn_bu_acre ~ year_id + year_id_sq, data = winneshiek_yields) ## summary(lm_yield_year_quad) ## save fitted values winneshiek_yields$fitted_quad &lt;- lm_yield_year_quad$fitted.values ## compare models anova_m1_m2 &lt;- anova(lm_yield_year, lm_yield_year_quad) ## anova_m1_m2[2, &quot;Pr(&gt;F)&quot;] ## label model in ggplot model_label_quad &lt;- as.character(as.expression( substitute( italic(y) == a + b %.% italic(year) + c %.% ~italic(year)^2~&quot;,&quot;~~italic(r)^2~&quot;=&quot;~r2 , list( a = scales::comma(summary(lm_yield_year_quad)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;], accuracy = 0.1) , b = scales::comma(summary(lm_yield_year_quad)$coefficients[2, &quot;Estimate&quot;], accuracy = 0.2) , c = scales::comma(summary(lm_yield_year_quad)$coefficients[3, &quot;Estimate&quot;], accuracy = 0.0001) , r2 = format(summary(lm_yield_year_quad)$r.squared, digits = 2) ) ) )) ## model_label_quad ## plot ggplot(winneshiek_yields, aes(x = year_id, y = yield_corn_bu_acre)) + geom_point(alpha=0.8, color = &quot;navy&quot;, size = 2) + geom_line(aes(x = year_id, y = fitted_quad), color = &quot;grey35&quot;) + annotate(geom = &quot;text&quot;, label = model_label_quad, parse = TRUE, x = -Inf, y = Inf, hjust = -0.1, vjust = 2) + scale_x_continuous(breaks = scales::extended_breaks(n=10)) + labs( title = &quot;Corn Yield over time in Winneshiek County, Iowa&quot; , subtitle = &quot;with fitted values from quadratic time trend (i.e., year + year^2)&quot; ) + xlab(&quot;Year&quot;) + ylab(&quot;Corn Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) Using a quadratic time trend (i.e., year + year^2) shows a significant time trend in the corn yields in Winneshiek County, Iowa from 1981 to 2021. Based on a the quadratic model, there is a slightly increasing rate of corn yield over time. The quadratic time trend explains 76% of the variation in corn yields in Winneshiek County, Iowa. Using the anova() function to compare the quadratic model with the simple linear model allows for us to test which model provides the best parsimonious fit of the data. Although the quadratic model is significant, the ANOVA test has a resulting p-value of 0.723. This result means that adding the quadratic term did not significantly improve the model over the simple linear model. 7.0.3.3 Question 2 Time Series: Let’s analyze the relationship between temperature and yields for the Winneshiek County time series. Use data on yield and summer avg Tmax. Is adding year or Tmax^2 to your model helpful? Make a plot and interpret the results. 7.0.3.3.1 Yields on Temperature #join summer temperature to yield data for Winneshiek County winneshiek_yields_temp &lt;- inner_join(winneshiek_yields, winnesummer, by = c(&quot;year_id&quot;= &quot;year&quot;)) ## yield on temperature lm_yield_tmax &lt;- lm(yield_corn_bu_acre ~ meantmax, data = winneshiek_yields_temp) lm_yield_yr &lt;- lm(yield_corn_bu_acre ~ year_id, data = winneshiek_yields_temp) ## summary(lm_yield_tmax) ## compare models anova_m1_m3 &lt;- anova(lm_yield_yr, lm_yield_tmax) ### plot ggplot(winneshiek_yields_temp, aes(x = meantmax, y = yield_corn_bu_acre)) + geom_point(alpha=0.8, color = &quot;navy&quot;, size = 2) + geom_smooth(method = &#39;lm&#39;, color = &quot;grey35&quot;) + scale_x_continuous(breaks = scales::extended_breaks(n=10)) + labs( title = &quot;Corn Yield versus Mean Summer Max. Temperature in Winneshiek County, Iowa&quot; ) + xlab(&quot;Summer Max. Temp. (°C)&quot;) + ylab(&quot;Corn Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) A simple linear regression of corn yield on mean summer maximum temperature shows that temperature alone explains 3% of the variation in corn yields in Winneshiek County, Iowa. This model performs worse than the simple time trend of annual corn yield investigated above. 7.0.3.3.2 Yields on Year &amp; Temperature ## yield on temperature and year lm_yield_year_tmax &lt;- lm(yield_corn_bu_acre ~ year_id + meantmax, data = winneshiek_yields_temp) ## summary(lm_yield_year_tmax) ## compare models anova_m1_m4 &lt;- anova(lm_yield_yr, lm_yield_year_tmax) anova_m1_m4 ## Analysis of Variance Table ## ## Model 1: yield_corn_bu_acre ~ year_id ## Model 2: yield_corn_bu_acre ~ year_id + meantmax ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 36 10752 ## 2 35 10192 1 560.03 1.9231 0.1743 ## linear function holding tmax constant at mean lm_yield_year_tmax_eq1 &lt;- function(x){summary(lm_yield_year_tmax)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;] + (summary(lm_yield_year_tmax)$coefficients[2, &quot;Estimate&quot;] * x) + (summary(lm_yield_year_tmax)$coefficients[3, &quot;Estimate&quot;] * mean(winneshiek_yields_temp$meantmax))} #label model_label_yr_tmax &lt;- as.character(as.expression( substitute( italic(y) == a + b %.% italic(year) + c %.% ~italic(tmax)~&quot;,&quot;~~italic(r)^2~&quot;=&quot;~r2 , list( a = scales::comma(summary(lm_yield_year_tmax)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;], accuracy = 1) , b = scales::comma(summary(lm_yield_year_tmax)$coefficients[2, &quot;Estimate&quot;], accuracy = 0.2) , c = scales::comma(summary(lm_yield_year_tmax)$coefficients[3, &quot;Estimate&quot;], accuracy = 0.2) , r2 = format(summary(lm_yield_year_tmax)$r.squared, digits = 2) ) ) )) #plot ggplot(winneshiek_yields_temp, aes(x = year_id, y = yield_corn_bu_acre)) + geom_point(alpha=0.8, color = &quot;navy&quot;, size = 2) + stat_function(fun = lm_yield_year_tmax_eq1 , geom=&quot;line&quot; , show.legend = FALSE , color = &quot;grey35&quot; ) + annotate(geom = &quot;text&quot;, label = model_label_yr_tmax, parse = TRUE, x = -Inf, y = Inf, hjust = -0.1, vjust = 2) + scale_x_continuous(breaks = scales::extended_breaks(n=10)) + labs( title = &quot;Corn Yield over time in Winneshiek County, Iowa&quot; , subtitle = &quot;with fitted values of yield over time while holding max. temperature constat at mean&quot; ) + xlab(&quot;Year&quot;) + ylab(&quot;Corn Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) A multiple linear regression of corn yield on year and mean summer maximum temperature explains 75% of the variation in corn yields in Winneshiek County, Iowa. Using the anova() function to compare the model including year and temperature with the simple linear model including only year allows for us to test which model provides the best parsimonious fit of the data. The ANOVA test has a resulting p-value of 0.174. This result means that adding the mean summer maximum temperature did not significantly improve the model over the simple linear model on year alone. 7.0.3.3.3 Yields on Year &amp; Temperature quadratic ## yield on temperature quadratic and year winneshiek_yields_temp &lt;- winneshiek_yields_temp %&gt;% mutate(meantmax_sq = meantmax^2) lm_yield_year_tmax_sq &lt;- lm(yield_corn_bu_acre ~ year_id + meantmax + meantmax_sq, data = winneshiek_yields_temp) ## summary(lm_yield_year_tmax_sq) ## compare models anova_m1_m5 &lt;- anova(lm_yield_yr, lm_yield_year_tmax_sq) anova_m1_m5 ## Analysis of Variance Table ## ## Model 1: yield_corn_bu_acre ~ year_id ## Model 2: yield_corn_bu_acre ~ year_id + meantmax + meantmax_sq ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 36 10752.3 ## 2 34 6923.8 2 3828.5 9.4002 0.0005627 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## linear function holding tmax constant at mean lm_yield_year_tmax_sq_eq1 &lt;- function(x){summary(lm_yield_year_tmax_sq)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;] + (summary(lm_yield_year_tmax_sq)$coefficients[2, &quot;Estimate&quot;] * x) + (summary(lm_yield_year_tmax_sq)$coefficients[3, &quot;Estimate&quot;] * mean(winneshiek_yields_temp$meantmax)) + (summary(lm_yield_year_tmax_sq)$coefficients[4, &quot;Estimate&quot;] * mean(winneshiek_yields_temp$meantmax)^2)} #label model_label_yr_tmax_sq &lt;- as.character(as.expression( substitute( italic(y) == a + b %.% italic(year) + c %.% ~italic(tmax) + d %.% ~italic(tmax)^2~&quot;,&quot;~~italic(r)^2~&quot;=&quot;~r2 , list( a = scales::comma(summary(lm_yield_year_tmax_sq)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;], accuracy = 1) , b = scales::comma(summary(lm_yield_year_tmax_sq)$coefficients[2, &quot;Estimate&quot;], accuracy = 0.2) , c = scales::comma(summary(lm_yield_year_tmax_sq)$coefficients[3, &quot;Estimate&quot;], accuracy = 0.2) , d = scales::comma(summary(lm_yield_year_tmax_sq)$coefficients[4, &quot;Estimate&quot;], accuracy = 0.2) , r2 = format(summary(lm_yield_year_tmax_sq)$r.squared, digits = 2) ) ) )) #plot ggplot(winneshiek_yields_temp, aes(x = year_id, y = yield_corn_bu_acre)) + geom_point(alpha=0.8, color = &quot;navy&quot;, size = 2) + stat_function(fun = lm_yield_year_tmax_sq_eq1 , geom=&quot;line&quot; , show.legend = FALSE , color = &quot;grey35&quot; ) + annotate(geom = &quot;text&quot;, label = model_label_yr_tmax_sq, parse = TRUE, x = -Inf, y = Inf, hjust = -0.02, vjust = 2) + scale_x_continuous(breaks = scales::extended_breaks(n=10)) + labs( title = &quot;Corn Yield over time in Winneshiek County, Iowa&quot; , subtitle = &quot;with fitted values of yield over time while holding max. temperature constat at mean&quot; ) + xlab(&quot;Year&quot;) + ylab(&quot;Corn Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) A multiple linear regression of corn yield on year and mean summer maximum temperature explains 83% of the variation in corn yields in Winneshiek County, Iowa. Using the anova() function to compare the model including year and temperature with the simple linear model including only year allows for us to test which model provides the best parsimonious fit of the data. The ANOVA test has a resulting p-value of 0.00056. This result means that adding the mean summer maximum temperature with a squared term did significantly improve the model over the simple linear model on year alone. Thus, of all of the models tested to estimate corn yields in Winneshiek County, Iowa, this model provides the best parsimonious fit of the data. 7.0.3.4 Question 3 Cross-Section: Analyze the relationship between temperature and yield across all counties in 2018. Is there a relationship? Interpret the results. ## summarize summer temperature data to county, year level county_year_tmax &lt;- tmaxdf %&gt;% mutate(county_ansi = as.numeric(as.character(countyfp))) %&gt;% filter(doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% group_by(county_ansi, year) %&gt;% summarize(meantmax = mean(tmax, na.rm = TRUE)) %&gt;% mutate(meantmax_sq = meantmax^2) ## join temp and yield data county_yr_full &lt;- inner_join(cornyields, county_year_tmax, by = c(&quot;county_ansi&quot; = &quot;county_ansi&quot;, &quot;year_id&quot; = &quot;year&quot;)) ## lost a few records without matching temperature data ## nrow(cornyields) - nrow(county_yr_full) county_yr_2018 &lt;- county_yr_full %&gt;% filter(year_id==2018) ## models lm_yield_tmax_2018 &lt;- lm(yield_corn_bu_acre ~ meantmax, data = county_yr_2018) lm_yield_tmax_sq_2018 &lt;- lm(yield_corn_bu_acre ~ meantmax + meantmax_sq, data = county_yr_2018) ## summary(lm_yield_tmax_2018) ## compare models anova_1_2 &lt;- anova(lm_yield_tmax_2018, lm_yield_tmax_sq_2018) anova_1_2 ## Analysis of Variance Table ## ## Model 1: yield_corn_bu_acre ~ meantmax ## Model 2: yield_corn_bu_acre ~ meantmax + meantmax_sq ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 91 35087 ## 2 90 31651 1 3436.7 9.7725 0.002385 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A simple linear regression of corn yield on mean summer maximum temperature for the counties in Iowa in 2018 explains 4% of the variation in corn yields. Adding a squared term for mean summer maximum temperature results in explaining 13% of the variation in corn yields for the counties in Iowa in 2018. We can also use the anova() function to compare the simple linear model to the quadratic model to test which model provides the best parsimonious fit of the data. The ANOVA test has a resulting p-value of 0.00239. This result means that adding the squared term for mean summer maximum temperature did significantly improve the model over the simple linear model. Thus, we will proceed with the quadratic model. ## save fitted values county_yr_2018$fitted_quad &lt;- lm_yield_tmax_sq_2018$fitted.values #label model_label_yr_tmax_sq &lt;- as.character(as.expression( substitute( italic(y) == a + b %.% ~italic(tmax) + c %.% ~italic(tmax)^2~&quot;,&quot;~~italic(r)^2~&quot;=&quot;~r2 , list( a = scales::comma(summary(lm_yield_tmax_sq_2018)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;], accuracy = 1) , b = scales::comma(summary(lm_yield_tmax_sq_2018)$coefficients[2, &quot;Estimate&quot;], accuracy = 0.2) , c = scales::comma(summary(lm_yield_tmax_sq_2018)$coefficients[3, &quot;Estimate&quot;], accuracy = 0.2) , r2 = format(summary(lm_yield_tmax_sq_2018)$r.squared, digits = 2) ) ) )) #plot ggplot(county_yr_2018, aes(x = meantmax, y = yield_corn_bu_acre)) + geom_point(alpha=0.8, color = &quot;navy&quot;, size = 2) + geom_line(aes(x = meantmax, y = fitted_quad), color = &quot;grey35&quot;) + annotate(geom = &quot;text&quot;, label = model_label_yr_tmax_sq, parse = TRUE, x = -Inf, y = Inf, hjust = -0.05, vjust = 1) + scale_x_continuous(breaks = scales::extended_breaks(n=10)) + scale_y_continuous(expand = c(0.1, 1)) + labs( title = &quot;2018 Corn Yield versus Mean Summer Max. Temperature for Counties of Iowa&quot; , subtitle = &quot;with fitted values from quadratic temperature model (i.e., tmax + tmax^2)&quot; ) + xlab(&quot;Summer Max. Temp. (°C)&quot;) + ylab(&quot;Corn Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) The model of corn yield on mean summer maximum temperature and its squared term for the counties in Iowa in 2018 explains 13% of the variation in corn yields. The trend shown above estimates a positive impact of temperature on corn yield with a maximum impact around 28 °C. After this point, increasing temperatures negatively impacts corn yields based on 2018 data for the counties in Iowa. 7.0.3.5 Question 4 Panel: One way to leverage multiple time series is to group all data into what is called a “panel” regression. Convert the county ID code (“countyfp” or “county_ansi”) into factor using as.factor, then include this variable in a regression using all counties’ yield and summer temperature data. How does the significance of your temperature coefficients (Tmax, Tmax^2) change? Make a plot comparing actual and fitted yields and interpret the results of your model. 7.0.3.5.1 Develop models ## make a factor of county code county_yr_full &lt;- county_yr_full %&gt;% mutate( name_ansi = paste0(county_name, &quot; (&quot;, as.character(county_ansi), &quot;)&quot;) , county_factor = as.factor(name_ansi) ) ## explore panel data ## coplot(yield_corn_bu_acre ~ year_id|county_factor, type=&quot;l&quot;, data=county_yr_full) ## Lines ## car::scatterplot(yield_corn_bu_acre~year_id|county_factor, data=county_yr_full, boxplots=FALSE, smooth=TRUE, reg.line=FALSE, legend=FALSE) ## gplots::plotmeans(yield_corn_bu_acre ~ county_factor, main=&quot;Heterogeineity across counties&quot;, data=county_yr_full, n.label=FALSE) ## gplots::plotmeans(yield_corn_bu_acre ~ year_id, main=&quot;Heterogeineity across years&quot;, data=county_yr_full, n.label=FALSE) ## model yield with panel data ## Regular OLS regression does not consider heterogeneity across groups or time ## need to add a factor to account for differences across groups ## yield on year lm_pnl_yield_year &lt;- lm(yield_corn_bu_acre ~ year_id + county_factor, data = county_yr_full) ## summary(lm_pnl_yield_year) ## yield on tmax lm_pnl_yield_tmax &lt;- lm(yield_corn_bu_acre ~ meantmax + county_factor, data = county_yr_full) ## summary(lm_pnl_yield_tmax) ## anova test anv_m1_m2 &lt;- anova(lm_pnl_yield_year, lm_pnl_yield_tmax) ## anv_m1_m2 ## (anv_m1_m2[2, &quot;Sum of Sq&quot;]) ## decrease in RSS means that model fits worse ## yield on tmax quadratic lm_pnl_yield_tmax_quad &lt;- lm(yield_corn_bu_acre ~ meantmax + meantmax_sq + county_factor, data = county_yr_full) ## summary(lm_pnl_yield_tmax_quad) ## anova test anv_m1_m3 &lt;- anova(lm_pnl_yield_year, lm_pnl_yield_tmax_quad) ## anv_m1_m3 ## (anv_m1_m3[2, &quot;Sum of Sq&quot;]) ## decrease in RSS means that model fits worse ## yield on full model lm_pnl_yield_full &lt;- lm(yield_corn_bu_acre ~ year_id + meantmax + meantmax_sq + county_factor, data = county_yr_full) ## summary(lm_pnl_yield_full) ## anova test anv_m1_m4 &lt;- anova(lm_pnl_yield_year, lm_pnl_yield_full) anv_m1_m4 ## Analysis of Variance Table ## ## Model 1: yield_corn_bu_acre ~ year_id + county_factor ## Model 2: yield_corn_bu_acre ~ year_id + meantmax + meantmax_sq + county_factor ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 3648 1716689 ## 2 3646 1293407 2 423282 596.6 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Four models were compared above to determine the best fit model for corn yield in the counties of Iowa between 1981 and 2018 using fixed effects (OLS dummy variable) for county: * simple linear model of corn yield on year (r-squared = 63%). * simple linear model of corn yield on average maximum summer temperature (r-squared = 19%). * quadratic model of corn yield on average maximum summer temperature and its square (r-squared = 26%). * full model of corn yield on year and average maximum summer temperature and its square (r-squared = 72%). Using the anova() function, fixed effects models were compared to test which model provides the best parsimonious fit of the data. The ANOVA test between the simple linear model of corn yield on year and the full model has a resulting p-value of 0. This result means that the full model significantly improved the estimate over the simple linear model. Thus, we will proceed with the full model. 7.0.3.5.2 Plot best model ## save predicted values for best model county_yr_full$y_hat &lt;- lm_pnl_yield_full$fitted.values ## car::scatterplot(y_hat~year_id|county_factor, data=county_yr_full, boxplots=FALSE, smooth=TRUE, reg.line=FALSE, legend=FALSE) ## plot y versus y_hat #plot ggplot(county_yr_full, aes(x = y_hat, y = yield_corn_bu_acre)) + geom_point(alpha=0.7, color = &quot;navy&quot;, size = 2) + geom_abline(color = &quot;grey35&quot;) + annotate(geom = &quot;text&quot;, label = &quot;*line indicates perfect fit&quot;, x = -Inf, y = Inf, hjust = -0.05, vjust = 2, color = &quot;grey35&quot;) + scale_x_continuous(limits = c(min(county_yr_full$yield_corn_bu_acre)*.97 , max(county_yr_full$yield_corn_bu_acre)*1.02), breaks = scales::extended_breaks(n=10)) + scale_y_continuous(limits = c(min(county_yr_full$yield_corn_bu_acre)*.97 , max(county_yr_full$yield_corn_bu_acre)*1.02), breaks = scales::extended_breaks(n=10)) + labs( title = &quot;Corn Yield versus Predicted Corn Yield for Counties of Iowa&quot; , subtitle = &quot;with predicted values from full model&quot; ) + xlab(&quot;Predicted Corn Yield (bushels per acre)&quot;) + ylab(&quot;Corn Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) 7.0.3.6 Question 5 Soybeans: Download NASS data on soybean yields and explore either a time series relationship for a given county, the cross-sectional relationship for a given year, or a panel across all counties and years. 7.0.3.6.1 Load data ## pull data on soybean yield from nassqs_yields ## parameters to query on nassqs_auth(key = my_qs_api_key) params &lt;- list(commodity_desc = &quot;SOYBEANS&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) ## download soybeanyieldsall &lt;- nassqs_yields(params) ## clean and filter this dataset soybeanyieldsall$county_ansi &lt;- as.numeric(soybeanyieldsall$county_ansi) soybeanyieldsall$yield &lt;- as.numeric(soybeanyieldsall$Value) soybeanyields &lt;- soybeanyieldsall %&gt;% filter( !is.na(county_ansi) &amp; !is.na(yield) &amp; util_practice_desc == &quot;ALL UTILIZATION PRACTICES&quot; &amp; prodn_practice_desc == &quot;ALL PRODUCTION PRACTICES&quot; ) %&gt;% select(county_ansi, county_name, yield, year) soybeanyields &lt;- tibble(soybeanyields) %&gt;% rename(year_id = year , yield_soybean_bu_acre = yield ## bushels per acre ) 7.0.3.6.2 Develop models ## model ## join with winneshiek corn data winneshiek_yields &lt;- winneshiek_yields %&gt;% inner_join(soybeanyields, by=c(&quot;year_id&quot;=&quot;year_id&quot;, &quot;county_ansi&quot;=&quot;county_ansi&quot;)) %&gt;% select(-fitted_quad) ## trend model for yield growth lm_yield_year &lt;- lm(yield_soybean_bu_acre ~ year_id , data = winneshiek_yields) ## summary(lm_yield_year) ## quadratic trend model for yield growth lm_yield_year_quad &lt;- lm(yield_soybean_bu_acre ~ year_id + year_id_sq, data = winneshiek_yields) ## summary(lm_yield_year_quad) #anova anv_yr_yrsq &lt;- anova(lm_yield_year, lm_yield_year_quad) anv_yr_yrsq ## Analysis of Variance Table ## ## Model 1: yield_soybean_bu_acre ~ year_id ## Model 2: yield_soybean_bu_acre ~ year_id + year_id_sq ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 39 1200.1 ## 2 38 1199.7 1 0.38414 0.0122 0.9127 The anova() function was used to compare a simple linear model of soybean yield over time versus a quadratic time trend model to determine which model provides the best parsimonious fit of the data. The ANOVA test had a resulting p-value of 0.91. This result means that the quadratic time trend did not significantly improve the estimate over the simple linear model. Thus, we will proceed with the simple linear model. 7.0.3.6.3 Plot best model ## label model in ggplot model_label &lt;- as.character(as.expression( substitute( italic(y) == a + b %.% italic(year)*&quot;,&quot;~~italic(r)^2~&quot;=&quot;~r2 , list( a = scales::comma(summary(lm_yield_year)$coefficients[&quot;(Intercept)&quot;, &quot;Estimate&quot;], accuracy = 0.1) , b = scales::comma(summary(lm_yield_year)$coefficients[2, &quot;Estimate&quot;], accuracy = 0.1) , r2 = format(summary(lm_yield_year)$r.squared, digits = 2) ) ) )) ## model_label ## plot ggplot(winneshiek_yields, aes(x = year_id, y = yield_soybean_bu_acre)) + geom_point(alpha=0.9, color = &quot;cadetblue&quot;, size = 2) + geom_smooth(method = &#39;lm&#39;, color = &quot;grey35&quot;) + annotate(geom = &quot;text&quot;, label = model_label, parse = TRUE, x = -Inf, y = Inf, hjust = -0.1, vjust = 2) + scale_x_continuous(breaks = scales::extended_breaks(n=10)) + labs( title = &quot;Soybean Yield over time in Winneshiek County, Iowa&quot; ) + xlab(&quot;Year&quot;) + ylab(&quot;Soybean Yield (bushels per acre)&quot;) + theme_bw() + theme( legend.position=&quot;none&quot; ) Similar to corn yeild, there is a significant time trend in the soybean yields in Winneshiek County, Iowa from 1981 to 2021. Based on a simple linear regression, every additional year results in the soybean yield increasing by 0.58 bu/acre. The annual time trend explains 62% of the variation in soybean yields in Winneshiek County, Iowa. 7.0.3.7 Bonus Find a package to make a county map of Iowa displaying some sort of information about yields or weather. Interpret your map. ## pull in spatial county data to join on count FIPS code counties &lt;- us_counties() %&gt;% setNames(make.names(names(.), unique = TRUE)) %&gt;% ## for some reason the state_name field is duplicated mutate(county_ansi = as.numeric(countyfp)) %&gt;% filter(state_name == &quot;Iowa&quot;) %&gt;% st_transform(2163) ## EPSG:2163 = US National Atlas Equal Area ## mapview(counties) #aggregate yields to county cornyields_sum &lt;- cornyields %&gt;% group_by(county_ansi, county_name) %&gt;% arrange(county_ansi, year_id) %&gt;% summarize( mean_yield_corn_bu_acre = mean(yield_corn_bu_acre, na.rm=TRUE) , min_year = min(year_id, na.rm=TRUE) , max_year = max(year_id, na.rm=TRUE) , first_yield_corn_bu_acre = first(yield_corn_bu_acre) , last_yield_corn_bu_acre = last(yield_corn_bu_acre) ) %&gt;% mutate( yield_pct_chg = ((last_yield_corn_bu_acre - first_yield_corn_bu_acre) / first_yield_corn_bu_acre)*100 , n_years = max_year - min_year ) ## join to yields spatial_county_yields &lt;- inner_join(counties, cornyields_sum, by = &quot;county_ansi&quot;) ## map options mapviewOptions(legend.pos = &quot;bottomright&quot;, homebutton.pos = &quot;bottomleft&quot;) lbl &lt;- paste0(&quot;Avg. Corn Yield (bushels per acre) &quot; , as.character(min(spatial_county_yields$min_year)) ,&quot;-&quot; , as.character(max(spatial_county_yields$max_year)) ) ## map mapview(spatial_county_yields , zcol = &quot;mean_yield_corn_bu_acre&quot; , alpha.regions = 0.7 , map.types = &quot;OpenStreetMap&quot; , label = c(&quot;namelsad&quot;) , col.regions = mako(n = 7, direction = -1) , at = seq(110, 170, 10) , legend = TRUE , layer.name = lbl , popup = popupTable( spatial_county_yields , zcol = c( &quot;county_ansi&quot; , &quot;namelsad&quot; , &quot;mean_yield_corn_bu_acre&quot; ) , row.numbers = FALSE , feature.id = FALSE ) ) In the time period examined, the northern counties in Iowa have had higher average outputs in corn per acre than southern counties. O’Brien County had the highest average yield at 160 bushels per acre while Clarke County had the lowest average at 112 bushels per acre. 7.0.3.8 Bonus #2 Challenge question - map trends in corn yields by county across Iowa. Interpret your map. ## use percent change to map lbl2 &lt;- paste0(&quot;% Change Corn Yield (bushels per acre) &quot; , as.character(min(spatial_county_yields$min_year)) ,&quot;-&quot; , as.character(max(spatial_county_yields$max_year)) ) ## map mapview(spatial_county_yields , zcol = &quot;yield_pct_chg&quot; , alpha.regions = 0.7 , map.types = &quot;OpenStreetMap&quot; , label = c(&quot;namelsad&quot;) , col.regions = mako(n = 7, direction = -1) , at = seq(15, 140, 20) , legend = TRUE , layer.name = lbl2 , popup = popupTable( spatial_county_yields , zcol = c( &quot;county_ansi&quot; , &quot;namelsad&quot; , &quot;yield_pct_chg&quot; ) , row.numbers = FALSE , feature.id = FALSE ) ) In the time period examined, the far western counties in Iowa have had the most significant increases in corn yield per acre. Crawford County had the highest corn yield increase of 132% while Hamilton County had the lowest corn yield increase of 18%. "],["beginning-machine-learning.html", "Chapter 8 Beginning Machine Learning 8.1 Objective 8.2 Spatial autocorrelation grouping 8.3 Model development 8.4 Assignment", " Chapter 8 Beginning Machine Learning This analysis utilzes XGBoost, an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. A helpful introduction to utilizing XGBoost in R can be accessed here. 8.1 Objective An old friend told me today about a term called “Grad student descent” which is a play on words with the term gradient descent, an extremely common tool used to tune hyperparameters (and parameters) for machine learning models. Grad student descent then is when that method is simply having grad students search a parameter space manually in part to improve the model, and in part to learn how these algorithms work. Today we are trying to predict the concentrations of either chlorophyll a (CHLA) or total suspended sediment (TSS) in the Loire River France using satellite imagery. The grab sample data (someone grabbing water from a stream for analysis), comes from the French electricity agency who collects this data to meet their water quality standards. Basically the core idea here is that water color as captured in an image tells you something about what is in the water. If it’s green and bright (lots of light reflected from the water), then it may have a lot of algae (Chlorophyll a is a proxy for algal biomass). If it’s tan and bright, it may have lots of sediment. If it’s dark and blue it’s clear! This color information is captured in “bands” like red, blue, green etc… from the Landsat series of satellites, which have collected data over the world since the late 70s. For this analysis we are only using Landsat 5,7, and 8, so the data goes back to 1984. 8.1.1 Data read wq_sr &lt;- read_csv(&#39;data/wq_sr.csv&#39;) %&gt;% mutate(year = year(date)) 8.2 Spatial autocorrelation grouping The code below utilizes a rudimentary technique for addressing spatial autocorrelation. For a more robust solution, the UCLA Statistical Consulting Group provides a helpful guide for addressing spatial autocorrelation. In spatial data, it is often the case that some or all outcome measures exhibit spatial autocorrelation. This occurs when the relative outcomes of two points is related to their distance. When analyzing spatial data, it is important to check for autocorrelation. ## define group by spatial location grp_wq_sr &lt;- wq_sr %&gt;% distinct(site_no, long, lat) %&gt;% arrange(long, lat) %&gt;% mutate( pct_rank = row_number()/n() , grp = case_when( pct_rank &lt;= 0.25 ~ &quot;1&quot; , pct_rank &lt;= 0.5 ~ &quot;2&quot; , pct_rank &lt;= 0.75 ~ &quot;3&quot; , TRUE ~ &quot;4&quot; ) ) ## quick plot of sample sites ggplot(grp_wq_sr, aes(x=long, y=lat, color=grp)) + geom_point() + labs( title = &quot;Sample Site Locations&quot; , subtitle = &quot;grouped locations to minimize spatial autocorrelation in sampling&quot; ) + theme_few() + scale_color_viridis(discrete = TRUE) ## join to main data wq_sr &lt;- wq_sr %&gt;% inner_join(grp_wq_sr %&gt;% select(site_no, grp), by = c(&quot;site_no&quot;=&quot;site_no&quot;)) %&gt;% arrange(grp, site_no, date) %&gt;% group_by(grp) 8.3 Model development This is the fiddly bit. The goal? Tune hyperparameters to get the lowest RMSE and the best Measured/Predicted fit (points close to the red line). ## A vector that contains all the columns I want to keep. predictors &lt;- c( &quot;azimuth&quot; , &quot;blue&quot; , &quot;green&quot; , &quot;nir&quot; , &quot;red&quot; , &quot;swir1&quot; , &quot;swir2&quot; , &quot;zenith&quot; , &quot;NR&quot; , &quot;BR&quot; , &quot;GR&quot; , &quot;SR&quot; , &quot;BG&quot; , &quot;BN&quot; , &quot;BS&quot; , &quot;GS&quot; , &quot;GN&quot; , &quot;ndvi&quot; , &quot;ndwi&quot; , &#39;fai&#39; , &quot;hillshade&quot; , &quot;hue&quot; , &quot;bright_tot&quot; , &quot;bright&quot; ) #function to run model booster &lt;- function(df = wq_sr, pred=&#39;CHLA&#39;, title=&#39;Chlorophyll a&#39;, features = predictors){ non_nas &lt;- (!is.na(df[,pred])) #remove nas df = df[non_nas,] #Sample 60% of the data using groups defined above train &lt;- df %&gt;% group_by(grp) %&gt;% slice_sample(prop = .6) %&gt;% ungroup() #Keep only data that is not in train #How could we make this safer (spatiotemporal robustness) test &lt;- df %&gt;% anti_join(., train, by=&#39;index&#39;) %&gt;% ungroup() ## Actual boosting model ag_mod &lt;- xgboost(data=train %&gt;% dplyr::select(features) %&gt;% as.matrix(.), label = train %&gt;% pull(pred) %&gt;% log10(.), nrounds = 400, nthread = 4, eta = 0.1, gamma = 0.0, max_depth = 4, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 4, alpha = 0.1, num_parallel_tree = 1, print_every_n = 200 ) #Tune ntree, k, numcut, bands that you use, etc... #apply predictions. test &lt;- test %&gt;% mutate(bpred = 10^predict(ag_mod, test %&gt;% dplyr::select(features) %&gt;% as.matrix(.))) #Optional for log #test[,pred] = 10^test[,pred] #Remove NAs test &lt;- test %&gt;% filter(!is.na(pred)) %&gt;% as.data.frame() error &lt;- tibble(rmse=Metrics::rmse(test$bpred,test[,pred]), mdae=Metrics::mdae(test$bpred,test[,pred]), mape=Metrics::mape(test$bpred,test[,pred]), bias=Metrics::bias(test$bpred,test[,pred])) ## plot results g1 &lt;- ggplot(test, aes_string(x=pred,y=&#39;bpred&#39;,color=&#39;year&#39;)) + geom_point() + geom_abline(intercept=0,slope=1,color=&#39;red&#39;) + labs(x=&#39;Measured&#39;,y=&#39;Predicted&#39;) + theme_few() + scale_color_viridis_c() + scale_x_log10(breaks = trans_breaks(&quot;log10&quot;, function(x) 10^x), labels = trans_format(&quot;log10&quot;, math_format(10^.x))) + scale_y_log10(breaks = trans_breaks(&quot;log10&quot;, function(x) 10^x), labels = trans_format(&quot;log10&quot;, math_format(10^.x))) + ggtitle(title) ## returns a list of the plot (g1), the error metric (error), and the ## model (ag_mod) return(list(g1,error,ag_mod)) } 8.3.1 Chl-a model set.seed(81640) ### Makes the work reproducible, but you can hack this! ## call model function chl_boost &lt;- booster(df = wq_sr) 8.3.2 Chl-a Model Measured/Predicted Plot ## plot measured vs predicted chl_boost[[1]] 8.3.3 Chl-a Error Metrics ## view error metrics chl_boost[[2]] ## # A tibble: 1 × 4 ## rmse mdae mape bias ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27.3 5.63 1.15 -6.67 8.3.4 TSS Model ## total suspended solids model tss_boost &lt;- booster(df = wq_sr, pred = &quot;TSS&quot;, title=&quot;Total suspended solids (TSS)&quot;, features = predictors) 8.3.4.1 TSS plot ## plot TSS measured vs predicted tss_boost[[1]] 8.3.5 TSS Evaluation ## view error metrics tss_boost[[2]] ## # A tibble: 1 × 4 ## rmse mdae mape bias ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 97.4 5.11 0.628 -12.4 8.4 Assignment 8.4.1 1) What is the best CHLA RMSE you could get after 50 minutes of fiddling? After 50 minutes of fiddling with the xgboost parameters, the best RMSE achieved for predicting CHLA was 27.34. 8.4.2 2) What is the best TSS RMSE you could get after 50 minutes of fiddling? After 50 minutes of fiddling with the xgboost parameters, the best RMSE achieved for predicting TSS was 97.38. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
